{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yp5Nn7lzV3J6"
      },
      "source": [
        "# Load pytorch library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KtysNYMHV-XR"
      },
      "source": [
        "# Define validation dataset ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "valid_ratio = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m77sHXmJWHfx"
      },
      "source": [
        "# Define the MNIST training and validation sets, and possible transforms to be applied. Optional augmentation can be done within the transform. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n"
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "    #  transforms.RandomRotation(degrees=30),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_valid_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
        "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
        "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=500,\n",
        "                                          shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=500,\n",
        "                                          shuffle=True)\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "           '4', '5', '6', '7', '8', '9')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bEynLBaxWrnQ"
      },
      "source": [
        "# Visualize the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n    1     2     5     7\n"
        },
        {
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:4,]))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9CUKf7kcW3Rh"
      },
      "source": [
        "# Construct the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_FC-AEZXXJE4"
      },
      "source": [
        "# Instantiate the CNN and print out the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "884330\n"
        }
      ],
      "source": [
        "net = Net()\n",
        "print(sum([p.numel() for p in net.parameters()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "slDGVTtkXd-Z"
      },
      "source": [
        "# Define the loss function and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M9WLDB1R0CnA"
      },
      "source": [
        "# Select the device to train the CNN! \"cuda:0\" means the first GPU device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "cpu\n"
        },
        {
          "data": {
            "text/plain": "Net(\n  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=1568, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n)"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "La3a6S81YSWs"
      },
      "source": [
        "# Mount your google drive to current virtual machine. And define the path to store the trained CNN parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "PATH = 'mnist_net.pth'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8dLwz9XsYkNJ"
      },
      "source": [
        "# Train the CNN and store the best model based on the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch:  0  train loss:  0.056\nEpoch:  0  validation loss:  0.052\nEpoch:  1  train loss:  0.041\nEpoch:  1  validation loss:  0.056\nEpoch:  2  train loss:  0.038\nEpoch:  2  validation loss:  0.058\nEpoch:  3  train loss:  0.032\nEpoch:  3  validation loss:  0.053\nEpoch:  4  train loss:  0.029\nEpoch:  4  validation loss:  0.058\nEpoch:  5  train loss:  0.026\nEpoch:  5  validation loss:  0.053\nEpoch:  6  train loss:  0.019\nEpoch:  6  validation loss:  0.070\nEpoch:  7  train loss:  0.023\nEpoch:  7  validation loss:  0.064\nEpoch:  8  train loss:  0.022\nEpoch:  8  validation loss:  0.111\nEpoch:  9  train loss:  0.029\nEpoch:  9  validation loss:  0.060\nFinished Training in 15 mins\n"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "best_loss = np.float('inf')\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / (i+1)\n",
        "    print(\"Epoch: \", epoch, \" train loss: \", '%.3f' % epoch_loss)\n",
        "    with torch.no_grad(): \n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(validloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # forward \n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "      epoch_loss = running_loss / (i+1)\n",
        "      print(\"Epoch: \", epoch, \" validation loss: \", '%.3f' % epoch_loss)\n",
        "      if epoch_loss < best_loss:\n",
        "        torch.save(net.state_dict(), PATH)\n",
        "        best_loss = epoch_loss\n",
        "\n",
        "time_elap = (time.time() - start_time) // 60\n",
        "print('Finished Training in %d mins' % time_elap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yfvPe-jSYsrR"
      },
      "source": [
        "# Define the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "     [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4nqrJcR2Yzs5"
      },
      "source": [
        "# Visualize the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nGroundTruth:      7     2     1     0\n"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    \n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l8DMyi8-Y4nB"
      },
      "source": [
        "# Load the learned CNN parameters. This is required when you have trained the CNN and do no want to train it again by loading the learned parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fL7bMIiyZeFr"
      },
      "source": [
        "# Get the predictions for the first 4 images in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Predicted:      7     2     1     0\n"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  outputs = net(images.to(device))\n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3r7FPw9MZoMB"
      },
      "source": [
        "# Infer on the whole test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Accuracy of the network on the 10000 test images: 98.450 %\n"
        }
      ],
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
        "                                         shuffle=False, num_workers=1)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %.3F %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JOZILCzoZyQC"
      },
      "source": [
        "# check the GPU device assigned by Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "'nvidia-smi' is not recognized as an internal or external command,\noperable program or batch file.\n'ln' is not recognized as an internal or external command,\noperable program or batch file.\n"
        }
      ],
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getoutput('nvidia-smi'))"
      ]
    }
  ]
}